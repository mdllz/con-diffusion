---
title: "Creative or not, Procedure and model Study 2"
output:
  pdf_document: papaja::apa6_pdf
  html_document: default
  word_document: papaja::apa6_word
---

# Study 2

Based on what we had learned from Study 1 and based on previous research, we expected positive effects of stimulus originality (H1) and stimulus utility (H2) on the drift rate. We also expected the effect of stimulus originality to be larger than the effect of stimulus utility (H3). Furthermore, given the observed substantial negative correlation between the individual stimulus originality and utility effects in Study 1, we expected a negative correlation among those effects in Study 2 (H4). Specifically, we hypothesized that the greater an individual's effect of stimulus originality on the drift rate, the smaller the effect of stimulus utility would be and vice versa. Since in Study 1, we observed substantial individual differences in the extent to which stimulus originality and utility influenced the drift rate, we also expected non-zero variability across individuals in those effects (H5a and H6a).  
  Previous research suggests that originality plays a superior role in creativity judgments compared to utility [e.g., @CaroffBesancon:2008; @DiedrichEtAl:2015; @RuncoCharles:1993]. Based on these findings, we tested the hypothesis that everyone would have a positive effect of stimulus originality on the drift rate (H5b). Note that in Study 1, a few participants seemed to have a negative effect of originality. However, because this finding concerned only few participants and because we were not aware of any theory or research supporting it, we did not consider it robust enough to inform Study 2. Instead we decided to quantify the evidence for the ordinal constraint that everyone has a positive effect in Study 2 [@HaafRouder:2017; @HaafRouder:2018]. Regarding the effects of stimulus utility on the drift rate, we expected that some individuals would have a positive effect, some a negative effect, and some no effect (H6b).
Given Study 1’s results, we also expected that individuals’ stimulus originality and utility effects would be positively associated with their self-reported importance ratings of originality and utility for creativity. More specifically, we expected  individual originality effects on the drift rate would increase as the self-reported importance ratings of originality increase (H7a), and individual utility effects on the drift rate would increase as the importance ratings of utility increase (H7b).  
  Finally, we specified hypotheses regarding the association between CON-task judgements and AUT performance. We expected that originality scores on the AUT would be positively correlated with stimulus originality effects and negatively correlated with stimulus utility effects on drift rates (H8a and H8b). Similarly, we expected that AUT utility scores would be positively correlated with stimulus utility effects on CON-task drift rates and negatively correlated with stimulus originality effects (H9a and H9b). Note that Study 1 did not support H9a and H9b. However, in both cases, we did not consider the evidence for the null to be convincing. Based on common sense, we still expected that the more useful one’s AUT responses are, the more one values utility when judging creativity and the more one disregards originality -- also since studies suggest that the more creative someone is, the better they are at judging creativity [e.g., @BenedekEtAl:2016; @Silvia:2008]. Study 2 served as a robustness check for this belief.  


## Data Collection Procedure and Materials 

```{r}
fulldat2 <- read.csv("../Data/Study2/CON_S2.csv")
```

As in Study 1, data collection was centrally organized by the faculty of Psychology at the University of Amsterdam and took again place over different sessions. All tasks related to creativity were administered during the same session in the order listed below. In total, `r length(unique(fulldat2$pers))` first-year psychology students completed the CON-task, the age range was 17-47 years, (*M* = 20.50, *SD* = 2.98, and participants again received course credit for their participation.  

```{r 'S2 ICC AUT', include = F}
dat_aut2_icc <- read.csv("../Data/Study2/AUT_icc_S2.csv")

brick_icc_S2 <- compute_icc(dat_aut2_icc, "brick")
paperclip_icc_S2 <- compute_icc(dat_aut2_icc, "paperclip")
```

**Alternative-Uses-Task** Participants completed the AUT (Guilford, 1967) for the objects 'brick' and 'paperclip' and were again given two minutes for each object. Two independent raters who were unaware of the research questions/hypotheses of this study again separately scored participants' answers with respect to originality and utility on a five-point scale and coded invalid responses as zero. Interrater reliability as assessed by ICCs[^x] can be considered moderate to good: for the object 'brick', the ICC was `r brick_icc_S2[[1]]$brick_orig$value` 95%CI [`r brick_icc_S2[[1]]$brick_orig$lbound`, `r brick_icc_S2[[1]]$brick_orig$ubound`] for originality and `r brick_icc_S2[[1]]$brick_util$value` 95%CI [`r brick_icc_S2[[1]]$brick_util$lbound`, `r brick_icc_S2[[1]]$brick_util$ubound`] for utility. For the object 'paperclip the ICC was `r paperclip_icc_S2[[1]]$paperclip_orig$value` 95%CI [`r paperclip_icc_S2[[1]]$paperclip_orig$lbound`, `r paperclip_icc_S2[[1]]$paperclip_orig$ubound`] for originality and `r paperclip_icc_S2[[1]]$paperclip_util$value` 95%CI [`r paperclip_icc_S2[[1]]$paperclip_util$lbound`, `r paperclip_icc_S2[[1]]$paperclip_util$ubound`] for utility. As performance indicators, we again used the mean originality and mean utility score across raters, objects, and responses.    

**Creative-Or-Not task** Participants again completed the same 64 stimuli as the participants in Study 1.

**Self-reported importance-ratings** On four separate items, participants again indicated after the CON-task how important they thought originality, innovativeness, appropriateness, and utility were when deciding whether something is creative or not.

[^x]: As in Study 1, we used a twoway model and computed single score ICCs of the type consistency using the irr package (Gamer, Lemon, & Singh, 2019). Note that to compute the ICCs for the objects separately, we again used all collected AUT responses and not only the responses of the participants who also completed the CON task.
