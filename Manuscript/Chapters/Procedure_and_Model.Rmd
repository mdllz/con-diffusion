---
title: "Creative or not, Procedure and Model Study 1"
output:
  pdf_document: papaja::apa6_pdf
  html_document: default
  word_document: papaja::apa6_word
---
# General Method

## Bayesian Hierarchical Diffusion Modeling of the CON-task 
  The DDM conceptualizes the response process in the CON-task as an interaction of several unobservable cognitive processes [@Ratcliff:1978; @RatcliffMcKoon:2008; @VandekerckhoveEtAl:2011]. Each of these is represented by a parameter (see Figure \@ref(fig:DDMfigure)). We use the simplest complete version of the DDM comprising four parameters [@Ratcliff:1978; @RatcliffMcKoon:2008; @WabersichVandekerckhove:2014]. First, the model assumes that the decision whether a use is creative or not is initially determined by $\beta$, which reflects the a priori bias towards either choice, regardless of stimulus characteristics. Applied to the CON-task, this is an initial preference for “Yes, creative” or “No, not creative”. Second, according to the DDM, individuals gradually extract and accumulate noisy information from the stimulus regarding its creativity, which in turn determines the drift rate $\delta$, the tendency to respond 'creative' or 'not creative'. Positive values suggest a drift towards the upper boundary and negative ones a drift towards the lower boundary. Drift rates around zero suggest that a stimulus is perceived as ambiguous. The higher the absolute drift rate, the easier and faster the creativity evaluation, and the stronger the evidence for the decision. The evidence accumulation ends when either of the two decision boundaries is reached. Third, the boundary separation parameter $\alpha$ reflects the distance between the two response boundaries and can be interpreted as response caution, where more hesitant creativity judges have a greater boundary separation. Finally, the parameter $\tau$ refers to the non-decision time. This parameter captures the processes taking place before and after the actual decision process such as stimulus encoding and motor control processes.  
  In this paper, the most central DDM parameter is the drift rate. The assumption is that stimulus originality and utility both positively affect the drift rate in that the more original and useful a CON-task stimulus is, the greater the tendency to respond 'creative'. Moreover, the drift rate is the only model parameter that is influenced by stimulus characteristics because the remaining parameters are already set before the creativity evaluation starts [e.g., @VandekerckhoveEtAl:2011].
  
### Bayesian hierarchical modeling 
  We estimated the model in a Bayesian hierarchical framework [@Lee:2011; @RouderLu:2005; @VandekerckhoveEtAl:2011], allowing us to examine the data both at the population-level and at the individual-level. Hierarchical modeling provides rather conservative estimates of individual differences because it shrinks the individual effects towards the population mean [e.g., @EfronMorris:1977; @HaafRouder:2018].   
  We chose Bayesian estimation for three reasons. First, even without a hierarchical extension, applying the DDM to data is computationally expensive [e.g., @Tuerlinckx:2004]. Extending it hierarchically makes the model quickly intractable when using the frequentist approach of maximum likelihood estimation  [@VandekerckhoveEtAl:2011]. Second, Bayesian inference has several advantages such as an intuitive treatment of uncertainty regarding the model parameters [@Wagenmakers:2009]. Third, Bayesian hierarchical modeling is the preferred method for small trial numbers as simulation studies suggest that this method can recover individual variation relatively successfully even with small numbers of observations per participant [@RatcliffChilders:2015].  

## Model Specification

A detailed and complete model specification of the DDM used in Study 1 and 2 can be found in Appendix A. Here, we describe how we decomposed the drift rate parameter and the hierarchical structure of the model. To explore the influence of originality and utility when judging creativity in the CON-task, we regressed the drift rate on the originality and utility ratings of the stimuli. In both studies, we included random intercepts and random slopes to explore individual differences. Furthermore, because the response times and proportions of 'creative' responses vary considerably across the 64 CON-task stimuli, we also included random intercepts pertaining to the stimuli.   
  In both studies, we decomposed the drift rate as follows. Let $\delta_{(ij)}$ denote the drift rate for the $i$th participant, $i= 1, ..., I$, in the $j$th trial or stimulus, $j= 1, ...,64$, of the CON-task, then   

\[\delta_{(ij)} = \theta_{\delta(i)} + \phi_{\delta(j)} + \theta_{OR(i)} z_{OR(j)} + \theta_{UT(i)}z_{UT(j)}.\]

  The parameters $\theta_{\delta(i)}$, $\theta_{OR(i)}$, $\theta_{UT(i)}$, and $\phi_{\delta(j)}$ reflect the drift rate decomposition. Specifically, $\theta_{\delta(i)}$ denotes the drift rate intercept, representing individual $i$'s drift rate for stimuli with average originality and utility ratings. $\phi_{\delta(j)}$ is stimulus $j$'s deviation from the drift rate intercept. Furthermore, $z_{OR(j)}$ and $z_{UT(j)}$ are z-scores of the originality and utility of stimulus $j$. Lastly, $\theta_{OR(i)}$ denotes the originality effect, and $\theta_{UT(i)}$ the utility effect of individual $i$ on the drift rate.  
  For most of the remaining DDM parameters, we also incorporated random effects to examine individual differences. In particular, we allowed the boundary separation and the bias parameter to vary across individuals. However, because in Study 1 we encountered identifiability issues when estimating random effects for $\beta$, we fixed the bias at the population level. Another exception is that we constrained the non-decision parameter, $\tau$, to be constant across participants in both studies because interpreting random effects for this parameter has shown to be problematic [@Singmann:2018a].  
  To examine the interplay of the DDM parameters across participants, we also allowed the random effects pertaining to individuals to be correlated. As such, we assume that the individual effects are drawn from the same multivariate normal distribution with population means $[\mu_\delta, \mu_{\theta_{OR}}, \mu_{\theta_{UT}}, \mu_\alpha, \mu_\beta]^T$ and a variance-covariance matrix $\boldsymbol\Sigma$, i.e.,

\[\begin{bmatrix} 
\theta_{\delta(i)} \\
\theta_{OR(i)} \\
\theta_{UT(i)} \\
\alpha_{(i)} \\
\beta_{(i)}
\end{bmatrix} 
\sim
\mbox{Multivariate-Normal} \begin{pmatrix} 
\begin{bmatrix} 
\mu_\delta \\
\mu_{\theta_{OR}} \\
\mu_{\theta_{UT}} \\
\mu_\alpha \\
\mu_\beta
\end{bmatrix},
\boldsymbol{\Sigma}
\end{pmatrix}.
\]
  
  $\boldsymbol{\Sigma}$ allows for correlations across the random effects pertaining to the individuals. The random effects of the stimuli are orthogonal to the individual random effects. They are also assumed to be randomly sampled from a population distribution (of stimuli),
  
\[\phi_{\delta(j)} \sim \mbox{Normal}(0, \sigma_{\delta_{\phi}}),\]

where 0 is the mean and $\sigma_{\delta_{\phi}}$ is the standard deviation.  
  Since we estimated the model in the Bayesian framework, we needed to specify a prior distribution for each parameter. For Study 1, we used weakly informative priors that restricted the parameter space to a plausible range (see Appendix A). For Study 2, we used the insights gained from Study 1 and specified informative priors to test hypotheses in the Bayesian setting. We discuss these prior choices in the Methods, with details in Appendix A.  
  
# Study 1

## Data Collection Procedure and Materials 

```{r}
fulldat1 <- read.csv("../Data/Study1/CON_S1.csv")
```
 The data were gathered as part of a joint data collection effort by the Department of Psychology at the University of Amsterdam that took place over six sessions within one month. The sessions were filled with questionnaires and tasks from different researchers. Participation in each session was optional and participants received course credit. The full sample in our study consisted of `r length(unique(fulldat1$pers))` first year psychology students. The age range was 17-41 years (*M* = 20.38, *SD* = 2.59). Participants completed the tasks (i.e., the Alternative Uses Task and the CON-task) and questions all in one session in the order listed below[^s]. 

### Alternative Uses Task

```{r 'compute ICC AUT data S1', include = F}
dat_aut1_icc <- read.csv("../Data/Study1/AUT_icc_S1.csv")

compute_icc <- function(data, object){
  dat <- data %>% 
    filter(object == {{object}})
  
  # irr package
  icc_orig <- icc(ratings = cbind(dat$originality_rater01, dat$originality_rater02), 
                  model = "twoway",
                  type = "consistency",
                  unit = "single")
  icc_util <- icc(ratings = cbind(dat$utility_rater01, dat$utility_rater02), 
                  model = "twoway",
                  type = "consistency",
                  unit = "single")
  res_irr <- list(icc_orig = icc_orig, icc_util = icc_util)
  names(res_irr) <- c(paste0(object, "_orig"), paste0(object, "_util"))
  
return(list(res_irr))
}


brick_icc_S1 <- compute_icc(dat_aut1_icc, "brick")
towel_icc_S1 <- compute_icc(dat_aut1_icc, "towel")
fork_icc_S1 <- compute_icc(dat_aut1_icc, "fork")
paperclip_icc_S1 <- compute_icc(dat_aut1_icc, "paperclip")
```

 Participants completed a computerized version of the Alternative Uses Task [AUT; @Guilford:1967] used to assess their divergent thinking performance. The name of an object was presented on the screen, and participants had two minutes to type as many creative uses for the object as possible (e.g., the use 'bath toy' for the object 'brick'). During the session, participants were asked to generate uses for two objects, either "brick" and "fork", "fork" and "paperclip". or "paperclip" and "towel". The pairs were counter-balanced over participants. Generated solutions were listed on the screen and new ones were continuously added. Two independent raters who were unaware of the research questions/hypotheses of this study separately scored participants' answers with respect to originality and utility on a five-point scale (1 = not original/useful, 5 = very original/useful). Invalid responses were coded as 0. To assess interrater reliability, we computed the intraclass correlation coefficient (ICC)[^a]. The ICC for the originality scores were `r brick_icc_S1[[1]]$brick_orig$value` 95%CI [`r brick_icc_S1[[1]]$brick_orig$lbound`, `r brick_icc_S1[[1]]$brick_orig$ubound`], `r fork_icc_S1[[1]]$fork_orig$value` 95%CI [`r fork_icc_S1[[1]]$fork_orig$lbound`, `r fork_icc_S1[[1]]$fork_orig$ubound`], `r paperclip_icc_S1[[1]]$paperclip_orig$value` 95%CI [`r paperclip_icc_S1[[1]]$paperclip_orig$lbound`, `r paperclip_icc_S1[[1]]$paperclip_orig$ubound`] and `r towel_icc_S1[[1]]$towel_orig$value` 95%CI [`r towel_icc_S1[[1]]$towel_orig$lbound`, `r towel_icc_S1[[1]]$towel_orig$ubound`] for 'brick', 'fork', 'paperclip' and 'towel' respectively. For the utility scores, the corresponding ICCs were `r brick_icc_S1[[1]]$brick_util$value` 95%CI [`r brick_icc_S1[[1]]$brick_util$lbound`, `r brick_icc_S1[[1]]$brick_util$ubound`], `r fork_icc_S1[[1]]$fork_util$value` 95%CI [`r fork_icc_S1[[1]]$fork_util$lbound`, `r fork_icc_S1[[1]]$fork_util$ubound`], `r paperclip_icc_S1[[1]]$paperclip_util$value` 95%CI [`r paperclip_icc_S1[[1]]$paperclip_util$lbound`, `r paperclip_icc_S1[[1]]$paperclip_util$ubound`] and `r towel_icc_S1[[1]]$towel_util$value` 95%CI [`r towel_icc_S1[[1]]$towel_util$lbound`, `r towel_icc_S1[[1]]$towel_util$ubound`] for 'brick', 'fork', 'paperclip' and 'towel' respectively. As performance indicators, we used the mean originality and mean utility score across raters, objects, and responses.   

### Creative-or-not Task
```{r, include = F}
stimuli <- read.csv("../Data/stimuli.csv")
cor.orig.util <- correlationBF(stimuli$orig.rating, stimuli$util.rating, posterior = T, iterations = 10000)

BF.cor.orig.util <- correlationBF(stimuli$orig.rating, stimuli$util.rating)
```
  Participants completed `r nrow(stimuli)` trials of the CON-task. The instructions were in Dutch and read the following: "In a moment, you'll see other people's answers to the 'Creative Uses task'. We would like to know if you think the answers are creative or not creative. Decide as quickly as you can. We will do this task four times, each time with a different object (such as book). You will be shown 16 ideas for each object." On each trial, they were asked "Do you think this use for [object] is creative?", followed by a specific use. Importantly, participants were not instructed regarding the criteria they should apply when deciding whether they find a use creative or not. RTs as well as responses ('creative' or 'not creative') were recorded. Trials automatically counted as missing when participants did not answer within nine seconds. The stimuli used had been selected from a collection of AUT responses. Their originality and utility had been independently scored on a scale from 1 to 5 by two creativity researchers. The ICC was 0.88 95%CI [0.80, 0.92] for originality and 0.65 [.49, 0.77] for utility. As stimulus ratings, we took the average originality and utility rating, respectively, across raters. The mean originality rating of the stimuli was $M=$ `r mean(stimuli$orig.rating)` ($SD=$ `r sd(stimuli$orig.rating)`), and the mean utility rating was $M=$ `r mean(stimuli$util.rating)` ($SD=$ `r sd(stimuli$util.rating)`). Stimulus originality and utility were negatively correlated, $r =$ `r mean(cor.orig.util[,"rho"])`, 95% credible interval (CrI) [`r quantile(cor.orig.util, probs = 0.025)`, `r quantile(cor.orig.util, probs = 0.975)`]`r apa_print(BF.cor.orig.util)$full_result`[^l]. The correlation is representative of the trade-off between the two dimensions "originality" and "utility" that is often found in AUT responses [e.g.,@RietzschelEtAl:2010; @RuncoCharles:1993].

### Importance Ratings of Originality and Utility
  After the CON-task, participants indicated, separately, to what extent they thought utility, innovativeness, originality and appropriateness played a role when evaluating creativity (1 = not important at all to 5 = very important).
  
[^l]: Here and for all subsequently reported correlations, we conducted Bayesian correlation analyses using the Bayes factor package including the default prior scale (Morey & Rouder, 2018). Specifically, we used Bayes factors to quantify the evidence for a correlation ($H_1: \rho \neq 0$) as opposed to no correlation ($H_0: \rho = 0$; ) and report it together with the posterior mean of the correlation coefficient and the corresponding credible interval.

[^a]: Since we considered both responses and raters as random effects and considered consistency in ratings more important than absolute agreement, we used a twoway model and computed single score ICCs of the type consistency using the irr package throughout this paper (Gamer, Lemon, & Singh, 2019). Note that to compute the ICCs for the AUT objects separately, we used all collected AUT responses during the testing sessions and not only the responses of participants who also completed the CON-task.

[^s]: The Alternative Uses Task was administered in at least one other study during the six testing sessions but no other study made use of the CON-task.