---
title: "Creative or not, Main Analysis Study 1"
output:
  pdf_document: papaja::apa6_pdf
  html_document: default
  word_document: papaja::apa6_word
---
```{r 'read dat1 and clean and prepare'}
# load data
fulldat1 <- read.csv("../Data/Study1/CON_S1.csv")

##############
# clean data #
##############

## same response in at least 90% of the trials
excl1 <- fulldat1 %>%
  group_by(pers) %>%
  summarize(countCreative = sum(creative)/n(), 
            countNotCreative = 1-(sum(creative)/n())) %>%
  ungroup() %>% 
  filter(countCreative >= 0.9 | countNotCreative >= 0.9)

# exclude participants based on 90% criterion and remove first two responses
dat1_excl1 <- fulldat1 %>% 
  filter(!(pers %in% excl1$pers)) %>%
# remove first two trials
  group_by(pers) %>% 
  slice(3:n()) %>% 
  ungroup()

# remove RT > 6s and < 0.3s
dat1_excl2 <- dat1_excl1 %>% 
  filter(rt <= 6000  &  rt >= 300)

# exclude participants with fewer than 47 trials
excl3 <- dat1_excl2 %>% 
  group_by(pers) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n < 47) 

dat1 <- dat1_excl2 %>%
  filter(!(pers %in% excl3$pers)) %>% 
  # turn rt into seconds
  mutate(rt = rt/1000)
```



```{r 'S1 Initial RT and response analyses', include = F}
mean(dat1$rt)
sd(dat1$rt)
median(dat1$rt)
mean(dat1$creative)

mean(dat1[dat1$creative == 1,]$rt)
sd(dat1[dat1$creative == 1,]$rt)

mean(dat1[dat1$creative == 0,]$rt)
sd(dat1[dat1$creative == 0,]$rt)
```

```{r 'S1 correlation originality and utility and mean stim1 RT', include = F}
# median split including median
median_split_s1_util_low_incl_median <- dat1 %>% 
  mutate(util = ifelse(util.rating_s >= median(util.rating_s), 1, 0)) %>% 
  filter(util == 0) %>% 
  group_by(itemid) %>% 
  summarize(rt = mean(rt),
            util = unique(util),
            orig = unique(orig.rating_s)) %>% 
  ungroup()

median_split_s1_util_high_incl_median <- dat1 %>% 
  mutate(util = ifelse(util.rating_s >= median(util.rating_s), 1, 0)) %>% 
  filter(util == 1) %>% 
  group_by(itemid) %>% 
  summarize(rt = mean(rt),
            util = unique(util),
            orig = unique(orig.rating_s)) %>% 
  ungroup()

# correlation low utility: rt and originality  
bf_corr_median_split_s1_util_low_incl_median <- correlationBF(median_split_s1_util_low_incl_median$rt, median_split_s1_util_low_incl_median$orig)
##  BF_01
1/exp(bf_corr_median_split_s1_util_low_incl_median@bayesFactor$bf)
corr_median_split_s1_util_low_incl_median <-  posterior(bf_corr_median_split_s1_util_low_incl_median, iterations = 10000)
mean(corr_median_split_s1_util_low_incl_median[,1])


# correlation high utility: rt and originality  
bf_corr_median_split_s1_util_high_incl_median <- correlationBF(median_split_s1_util_high_incl_median$rt, median_split_s1_util_high_incl_median$orig)
exp(bf_corr_median_split_s1_util_high_incl_median@bayesFactor$bf)
corr_median_split_s1_util_high_incl_median <-  posterior(bf_corr_median_split_s1_util_high_incl_median, iterations = 10000)
mean(corr_median_split_s1_util_high_incl_median[,1])

# ------------------------------------------------------------

# median split excluding median
median_split_s1_util_low_excl_median <- dat1 %>% 
  mutate(util = ifelse(util.rating_s > median(util.rating_s), 1, 0)) %>% 
  filter(util == 0) %>% 
  group_by(itemid) %>% 
  summarize(rt = mean(rt),
            util = unique(util),
            orig = unique(orig.rating_s)) %>% 
  ungroup()

median_split_s1_util_high_excl_median <- dat1 %>% 
  mutate(util = ifelse(util.rating_s > median(util.rating_s), 1, 0)) %>% 
  filter(util == 1) %>% 
  group_by(itemid) %>% 
  summarize(rt = mean(rt),
            util = unique(util),
            orig = unique(orig.rating_s)) %>% 
  ungroup()

# correlation low utility: rt and originality  
bf_corr_median_split_s1_util_low_excl_median <- correlationBF(median_split_s1_util_low_excl_median$rt, median_split_s1_util_low_excl_median$orig)
1/exp(bf_corr_median_split_s1_util_low_excl_median@bayesFactor$bf)
corr_median_split_s1_util_low_excl_median <-  posterior(bf_corr_median_split_s1_util_low_excl_median, iterations = 10000)
mean(corr_median_split_s1_util_low_excl_median[,1])


# correlation high utility: rt and originality  
bf_corr_median_split_s1_util_high_excl_median <- correlationBF(median_split_s1_util_high_excl_median$rt, median_split_s1_util_high_excl_median$orig)
exp(bf_corr_median_split_s1_util_high_excl_median@bayesFactor$bf)
corr_median_split_s1_util_high_excl_median <-  posterior(bf_corr_median_split_s1_util_high_excl_median, iterations = 10000)
mean(corr_median_split_s1_util_high_excl_median[,1])

```

```{r 'paired t-test RT for creative and not creative response S1', include = F}
t_test_S1 <- dat1 %>% 
  group_by(pers, creative) %>% 
  summarize(mean_rt = mean(rt)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = creative, values_from = mean_rt) %>% 
  rename("not_creative" = "0", "creative" = "1") 

bf_rt_response_s1 <- ttestBF(x = t_test_S1$creative, y = t_test_S1$not_creative, paired=TRUE)
1/exp(bf_rt_response_s1@bayesFactor$bf)
```


```{r 'est settings S1'}
warmup1 <- 1500
iter1 <- 5000
chains1 <- 4
cores1 <- 4
max_td1 <- 15
adapt_d1 <- .97
```

```{r 'bring variables into right format for estimation'}
dat1 <- 
  dat1 %>% 
  mutate(pers = as.factor(pers),
         itemid = as.factor(itemid),
         creative = as.numeric(creative)) %>% 
  as.data.frame()
```


```{r 'model spec S1'}
formula <- bf(rt | dec(creative) ~ 1 + (1|itemid) + orig.rating_s + util.rating_s + (1 + orig.rating_s + util.rating_s|p|pers),
              bs ~ 0 + Intercept + (1|p|pers),
              bias ~ 1)

get_prior(formula,
          data = dat1, 
          family = wiener(link_bs = "identity",
                          link_ndt = "identity",
                          link_bias = "identity"))

prior <- c(
  prior("lkj(3)", class = "cor"),
  prior("normal(0, 1)", class = "b"),
  prior("normal(0, 0.3)", class = "sd"),
  set_prior("normal(0, 1)", class = "Intercept"),
  set_prior("beta(1.3, 1.3)", class = "Intercept", dpar = "bias"),
  set_prior("normal(0, 2)", class = "b", dpar = "bs", lb = 0),
  set_prior("normal(0, 0.3)", class = "sd", dpar =  "bs"),
  set_prior("normal(0, 0.3)", class = "sd", group = "pers"),
  set_prior("normal(0, 0.3)", class = "sd", group = "itemid"))


make_stancode(formula, 
              family = wiener(link_bs = "identity",
                              link_ndt = "identity",
                              link_bias = "identity"),
              data = dat1, 
              prior = prior)


initfun <- function() {
  list(
    b = rnorm(2, mean = 0, sd = 0.5),
    Intercept_bs = runif(tmp_dat$K_bs, 1, 2),
    ndt = 0.3,
    Intercept_bias = 0.5,
    L_2 = diag(tmp_dat$M_2),
    z_2 = matrix(rnorm(tmp_dat$M_2*tmp_dat$N_2, 0, 0.01),
                 tmp_dat$M_2, tmp_dat$N_2)
  )
}

tmp_dat <- make_standata(formula, 
                         family = wiener(link_bs = "identity",
                                         link_ndt = "identity",
                                         link_bias = "identity"),
                         data = dat1, prior = prior)

```

```{r 'estimate model S1', include = F, cache = TRUE}
wienermodel1 <- brm(formula, 
                   data = dat1,
                   family = wiener(link_bs = "identity", 
                                   link_ndt = "identity",
                                   link_bias = "identity"),
                   prior = prior, inits = initfun,
                   iter = iter1, warmup = warmup1, chains = chains1, cores = cores1, sample_prior = T,
                   control = list(max_treedepth = max_td1, adapt_delta = adapt_d1))

save(wienermodel1, file = "wienermodel1.Rda", compress = "xz")
```

```{r 'S1 model diagnostics', include = F}
# divergent transitions
nr.div.trans1 <- sum(subset(nuts_params(wienermodel1), Parameter=="divergent__")$Value)

# smallest number of eff1ective samples
head(sort(summary(wienermodel1$fit)$summary[,"n_eff"]))
min.neff1 <- min(summary(wienermodel1$fit)$summary[,"n_eff"])

# largest rhat-values
tail(sort(summary(wienermodel1$fit)$summary[,"Rhat"]))
```

```{r 'S1 model summary', include = F}
# general overview of posterior means of parameters
summary(wienermodel1)
```

```{r 'S1 FE results', include = F}
# fixed effects only
FEsummary1 <- brms::fixef(wienermodel1)
```

```{r 'S1 RE results', include = F}
# random effects only
REsummary1 <- summary(wienermodel1)$random$pers
```

```{r 'S1 individual posterior means drift rate', include = F}
# extract individual posterior means
eff1 <- coef(wienermodel1)$pers %>%
  as.data.frame() %>%
  rownames_to_column("pers") %>% 
  mutate(pers = as.numeric(pers))
```

```{r, include = F}
# correlation individual posterior means orig & util slopes
BF.cor.or.ut <- correlationBF(eff1$Estimate.orig.rating_s, eff1$Estimate.util.rating_s)
exp(BF.cor.or.ut@bayesFactor$bf)
cor.or.ut <-  posterior(BF.cor.or.ut, iterations = 10000)
mean(cor.or.ut[,1])
```


```{r 'S1 drift rate proportion CrI', include = F}
# Compute proportions of CrI above, around, and below zero

## Originality: CrI above 0
sum(eff1$Q2.5.orig.rating_s > 0)
round(sum(eff1$Q2.5.orig.rating_s > 0)/nrow(eff1)*100, digits = 2)

## Originality: CrI below 0
sum(eff1$Q97.5.orig.rating_s < 0)

## Originality: CrI includes 0
sum(eff1$Q2.5.orig.rating_s < 0 & eff1$Q97.5.orig.rating_s > 0)
round(sum(eff1$Q2.5.orig.rating_s < 0 & eff1$Q97.5.orig.rating_s > 0)/nrow(eff1)*100, digits = 2)

## Originality: posterior mean below 0
sum(eff1$Estimate.orig.rating_s < 0)
round(sum(eff1$Estimate.orig.rating_s < 0)/nrow(eff1)*100, digits = 2)

## Originality: posterior mean above 0
sum(eff1$Estimate.orig.rating_s > 0)
round(sum(eff1$Estimate.orig.rating_s > 0), digits = 2)


orig.dr.sums <- data.frame("CI above 0" = sum(eff1$Q2.5.orig.rating_s > 0)
                      , "CI below 0" = sum(eff1$Q97.5.orig.rating_s < 0)
                      , "CI includes 0" = sum(eff1$Q2.5.orig.rating_s < 0 & eff1$Q97.5.orig.rating_s > 0)
                      , "PM below 0" = sum(eff1$Estimate.orig.rating_s < 0)
                      , "PM above 0" = sum(eff1$Estimate.orig.rating_s > 0)
)

# Utility: CrI above 0
sum(eff1$Q2.5.util.rating_s > 0)
round(sum(eff1$Q2.5.util.rating_s > 0)/nrow(eff1)*100, digits = 2)

# Utility: CrI below 0
sum(eff1$Q97.5.util.rating_s < 0)
round(sum(eff1$Q97.5.util.rating_s < 0)/nrow(eff1)*100, digits = 2)

# Utility: CrI includes 0
sum(eff1$Q2.5.util.rating_s < 0 & eff1$Q97.5.util.rating_s > 0)
round(sum(eff1$Q2.5.util.rating_s < 0 & eff1$Q97.5.util.rating_s > 0)/nrow(eff1)*100, digits = 2)

# Utility: posterior mean below 0
sum(eff1$Estimate.util.rating_s < 0)
round(sum(eff1$Estimate.util.rating_s < 0)/nrow(eff1)*100, digits = 2)

# Utility: posterior mean above 0
sum(eff1$Estimate.util.rating_s > 0)
round(sum(eff1$Estimate.util.rating_s > 0)/nrow(eff1)*100, digits = 2)


util.dr.sums <- data.frame("CI above 0" = sum(eff1$Q2.5.util.rating_s > 0)
                      , "CI below 0" = sum(eff1$Q97.5.util.rating_s < 0)
                      , "CI includes 0" = sum(eff1$Q2.5.util.rating_s < 0 & eff1$Q97.5.util.rating_s > 0)
                      , "PM below 0" = sum(eff1$Estimate.util.rating_s < 0)
                      , "PM above 0" = sum(eff1$Estimate.util.rating_s > 0))
```

```{r 'S1 compute ICC AUT'}
dat_aut1_icc <- read.csv("../Data/Study1/AUT_icc_S1.csv")

compute_icc <- function(data, object){
  dat <- data %>% 
    filter(object == {{object}})
  
  # irr package
  icc_orig <- icc(ratings = cbind(dat$originality_rater01, dat$originality_rater02), 
                  model = "twoway",
                  type = "consistency",
                  unit = "single")
  icc_util <- icc(ratings = cbind(dat$utility_rater01, dat$utility_rater02), 
                  model = "twoway",
                  type = "consistency",
                  unit = "single")
  res_irr <- list(icc_orig = icc_orig, icc_util = icc_util)
  names(res_irr) <- c(paste0(object, "_orig"), paste0(object, "_util"))
  
return(list(res_irr))
}


brick_icc_S1 <- compute_icc(dat_aut1_icc, "brick")
towel_icc_S1 <- compute_icc(dat_aut1_icc, "towel")
fork_icc_S1 <- compute_icc(dat_aut1_icc, "fork")
paperclip_icc_S1 <- compute_icc(dat_aut1_icc, "paperclip")

```


```{r 'S1 correlations CON and AUT', include = F, results = 'hide'}
dat_AUT1_r <- read.csv("../Data/Study1/AUT_S1.csv")

# only keep AUT data from participants with DDM estimates
dat_AUT1_c <- dat_AUT1_r %>% 
  filter(pers %in% eff1$pers)

# clean data
valid_or_not_1 <- dat_AUT1_c %>% 
  # invalid responses: scored as 0 by either of the raters
  mutate(valid = ifelse((originality_rater01 == 0 & utility_rater01 == 0) | (originality_rater02 == 0 & utility_rater02 == 0), 0, 1)) %>% 
  group_by(pers) %>% 
  summarize(prop_valid = sum(valid)/n()) %>% 
  ungroup() %>% 
  arrange(prop_valid)

# save ids of participants with at least 90% valid responses
id_valid_1 <- valid_or_not_1 %>% 
  filter(prop_valid >= .9) %>% 
  pull(pers)

dat_AUT1 <- dat_AUT1_c %>% 
  # only keep 'valid' participants
  filter(pers %in% id_valid_1) %>% 
  # compute mean orig & util scores across raters and objects
  mutate(originality = (originality_rater01 + originality_rater02)/2,
         utility = (utility_rater01 + utility_rater02)/2) %>% 
  # exclude responses where both raters gave a 0
  filter(!(originality == 0 & utility == 0)) %>% 
  group_by(pers) %>% 
  summarize(origScore = mean(originality),
            utilScore = mean(utility)) %>% 
  ungroup()

eff1_aut <- left_join(eff1, dat_AUT1) %>% 
  # remove NAs (DDM estimate but no AUT score)
  drop_na()

# correlation AUT-originality and CON-originality
cor.autor.or <- correlationBF(eff1_aut$Estimate.orig.rating_s, eff1_aut$origScore, posterior = T, iterations = 10000)
mean(cor.autor.or[,"rho"])

BF.cor.autor.or <- correlationBF(eff1_aut$Estimate.orig.rating_s, eff1_aut$origScore)
exp(BF.cor.autor.or@bayesFactor$bf)

# correlation AUT-originality and CON-utility
cor.autor.ut <- correlationBF(eff1_aut$Estimate.util.rating_s, eff1_aut$origScore, posterior = T, iterations = 10000)
mean(cor.autor.ut[,"rho"])

BF.cor.autor.ut <- correlationBF(eff1_aut$Estimate.util.rating_s, eff1_aut$origScore)
exp(BF.cor.autor.ut@bayesFactor$bf)

# correlation AUT-utility and CON-utility
cor.autut.ut <- correlationBF(eff1_aut$Estimate.util.rating_s, eff1_aut$utilScore, posterior = T, iterations = 10000)
mean(cor.autut.ut[,"rho"])

BF.cor.autut.ut <- correlationBF(eff1_aut$Estimate.util.rating_s, eff1_aut$utilScore)
1/exp(BF.cor.autut.ut@bayesFactor$bf)

# correlation AUT utility and CON-originality
cor.autut.or <- correlationBF(eff1_aut$Estimate.orig.rating_s, eff1_aut$utilScore, posterior = T, iterations = 10000)
mean(cor.autut.or[,"rho"])

BF.cor.autut.or <- correlationBF(eff1_aut$Estimate.orig.rating_s, eff1_aut$utilScore)
1/exp(BF.cor.autut.or@bayesFactor$bf)
```



```{r 'S1 correlations CON and importance ratings', include = F}
dat_SR1_r <- read.csv("../Data/Study1/SR_ratings_S1.csv")

dat_SR1 <- left_join(eff1, dat_SR1_r, by = "pers") %>% 
  drop_na() %>% 
  mutate(origSum = origRating + innovRating,
         utilSum = utilRating + appropRating)

## originality slopes & originality importance ratings
cor.orrat.or <- correlationBF(dat_SR1$Estimate.orig.rating_s, dat_SR1$origSum, posterior = T, iterations = 10000)
mean(cor.orrat.or[,"rho"])

BF.cor.orrat.or <- correlationBF(dat_SR1$Estimate.orig.rating_s, dat_SR1$origSum)
exp(BF.cor.orrat.or@bayesFactor$bf)


## utility slopes & utility importance ratings
cor.utrat.ut <- correlationBF(dat_SR1$Estimate.util.rating_s, dat_SR1$utilSum, iterations = 10000, posterior = T)
mean(cor.utrat.ut[,"rho"])
BF.cor.utrat.ut <- correlationBF(dat_SR1$Estimate.util.rating_s, dat_SR1$utilSum)

exp(BF.cor.utrat.ut@bayesFactor$bf)


## utility slopes & originality importance ratings
cor.orrat.ut <- correlationBF(dat_SR1$Estimate.util.rating_s, dat_SR1$origSum, iterations = 10000, posterior = T)
mean(cor.orrat.ut[,"rho"])

BF.cor.orrat.ut <- correlationBF(dat_SR1$Estimate.util.rating_s, dat_SR1$origSum)
exp(BF.cor.orrat.ut@bayesFactor$bf)


## originality slopes & utility importance ratings

cor.utrat.or <- correlationBF(dat_SR1$Estimate.orig.rating_s, dat_SR1$utilSum, iterations = 10000, posterior = T)
mean(cor.utrat.or[,"rho"])

BF.cor.utrat.or <- correlationBF(dat_SR1$Estimate.orig.rating_s, dat_SR1$utilSum)
exp(BF.cor.utrat.or@bayesFactor$bf)
```

